# Overview

The process of extracting relations from sentences involves a training and testing step. For a more detailed overview of this process, see the [Relation Extraction page](relation_extraction.html). Distant supervision aims to automate the process of generating training examples to be used as input to the training step.

# Distant Supervision

### Overview

Recall our example from the relation extraction section: given the input sentence "Barack Obama was a student at Columbia University" and the target relation Alma_Mater(person, university), we want to extract the relation Alma_Mater(Barack_Obama, Columbia_University). 
The first step of relation extraction is to obtain labeled training examples so that the relation-extraction system can analyze the patterns in those examples. Each example is a mention pair extracted from a sentence, and must be annotated with the relation of which it is a positive instance, or with the fact that it is not a positive instance of any relation. Traditionally these labelings have been manually generated by humans, which is expensive and time-consuming. The goal of distant supervision is to automatically create a large number of positive and negative training examples to alleviate the need for human annotation.

### Drawbacks of Human Annotation

Obtaining human-labeled training examples is difficult to obtain in large amounts. Moreover, since relations are usually labeled on a specific text corpus by domain experts, the resulting relation-extraction systems contain bias toward the input (perform well on test examples that are in the same domain as the training text, and poorly on examples from other domains). To solve this bias would require obtaining training examples from a very large amount of text documents from diverse domains, which is not possible with human annotation. Thus, new techniques such as distant supervision must be investigated.

### Seed Facts

The input to distant supervision is an unlabeled text corpus, a target relation, and a set of known instances of the target relation called seed facts. An example of a seed fact for the target relation Alma_Mater(person, university) is Alma_Mater(Michelle_Obama, Princeton_University). These seed facts can come from various sources: for example, they can be generated by the user depending on the application (e.g. known interactions between proteins), or could be obtained from a knowledge base such as Wikipedia (such as the Alma_Mater(Michelle_Obama, Princeton_University) example). The purpose of seed facts is to act as the starting points for generating training examples from the text.

### Generating Positive Examples

Given a body of text, a target relation, and a set of seed facts involving pairs of entities participating in the target relation, distant supervision looks through sentences in the text corpus to find mentions involving mentions that link to both entities from each seed fact (see entity linking definition on the Relation Extraction page). If it finds both such mentions in the sentence, then the mention pair is automatically labeled as a positive example for the target relation. For example, given the seed fact Alma_Mater(Michelle_Obama, Princeton_University) and the target relation Alma_Mater(person, university), the mention pair ("Michelle Obama", "Princeton University") in the sentence "Michelle Obama went to school at Princeton University" would be recognized as a positive training example for Alma_Mater.

### Generating Negative Examples

We need both positive and negative training examples to create a reliable relation-extraction system. Once the positive examples for a given target relation are labeled, one way to generate the negative examples for that target relation is to use other relations. The negative examples are simply mention pairs that contain mentions of entities participating in all the other relations in the knowledge base except for the current target relation. In other words, a training example being labeled as positive for a particular relation means that it is to be labeled as negative for all the other relations in the knowledge base.

### Errors

While distant supervision is able to produce large amounts of training data from only a set of seed facts, the training data it produces contains some incorrect labels. This is expected because not every mention pair will be a positive instance of a single relation. For example, for the above seed fact, the mention pair ("Michelle Obama", "Princeton University") in the sentence "Michelle Obama applied to Princeton University" will be considered a positive example the mentions link to the entities Michelle_Obama and Princeton_University, even though the particular mention pair in that sentence is not indicative of Michelle Obama graduating from Princeton.
The idea that makes distant supervision useful in practice is the hypothesis that broad coverage (text contains lots of mention pairs that correspond to many different target relations) and high redundancy (assuming a seed fact is true, there will be many correct instances involving the 2 entities and fewer incorrect instances) in a large text corpus would compensate for this noise. For example, with a large enough corpus, a distant supervision system may discover that patterns involving mentions in the sentence "Michelle Obama went to school at Princeton University" strongly correlate with seed facts of Attended_University whereas patterns involving mentions in the sentence "Michelle Obama applied to Princeton University" do not qualify as strong indicators. Thus, the quality of distant supervision should improve as larger bodies of text are used.

### Using Distant Supervision

Given a text corpus and a knowledge base containing entities (Barack_Obama, Columbia_University, etc.), a target relation such as Alma_Mater(person, university), and seed facts such as Alma_Mater(Barack_Obama, Columbia_University), distant supervision generates positive and negative training examples. For each seed fact and associated target relation we map each of the entity pairs to mentions pairs in sentences of the text (we only consider pairs of mentions in the same sentence), and label those mention pairs as positive examples of the target relation. We then label those mention pairs as negative examples for all the other target relations.

For the target relation Alma_Mater(Michelle_Obama, Princeton_University), both sentences "Michelle Obama applied to Princeton University" and "Michelle Obama went to school at Princeton University" contain mention pairs that link to (Michelle_Obama, Princeton_University) entities, and so these mention pairs would be labeled as positive examples for the given target relation.

Once the positive and negative training examples are obtained, they are used as a training set for a relation-extraction system. The system would take as input another text corpus containing mention pairs in sentences, identify the entities within those mention pairs, extract features from the sentences containing the mention pairs, and for each relation in the knowledge base, see whether the extracted features indicate that the mention pair is a positive or negative instance of that relation.

# References

http://hazy.cs.wisc.edu/hazy/papers/acl2012.pdf
http://www.stanford.edu/~jurafsky/mintz.pdf
http://www.stanford.edu/class/cs124/lec/rel.pdf